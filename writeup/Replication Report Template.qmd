---
title: "Replication of Experiment 1 by Child, Oakhill, & Garnham (2018, Language, Cognition and Neuroscience)"
author: "Verity Lua (vyqlua@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
---
<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->
  
## Introduction
  
Does the ease of processing emotional information differ based on the emotional valence of the information and the perspective by which the emotional information is written? In Study 1 of the [paper](https://github.com/psych251/child2018_rescue/blob/main/original_paper/original_paper.pdf) "You're the emotional one: the role of perspective for emotion processing in reading comprehension", Child, Oakhill, & Garnham (2018) found that participants read positively-valenced passages faster when they were written from a personal (second person's) perspective compared to when they were written from an onlooker's (third person's) perspective. In contrast, there were no difference in reading times for negatively-valenced passages written from either perspective. However, a [replication study](https://github.com/psych251/child2018/blob/master/writeup/Child-et-al--2018--Replication-Report.html) done by Bunderson (2020) did not find a significant difference in reading time between positively-valenced passages written from a personal perspective and positively-valenced passages written from an onlooker's perspective. 
  
Hence, [the current project](https://github.com/psych251/child2018_rescue) seeks to examine the possible reason(s) why the findings failed to replicate. As a researcher in the field of affective science, how people process emotions is something that I am particularly interested in. Furthermore, as a social psychologist, I enjoy uncovering thinking about social factors that could explain why certain effects may be more pronounced in certain contexts and samples compared to others. I suspect that the effect found by Child et al. (2018) may be moderated by educational status, whereby the effect may more pronounced among highly educated individuals who may have greater exposure to reading and writing personal anecdotes throughout their educational journey. Of importance, while Child et al. (2018) carried out their experiment using a sample from the subject pool of the University of Sussex, Bunderson (2020) utilized a sample recruited from Amazon Mturk. Thus, educational status may be a potential variable that could allow us to understand the conflicting findings.

The current project will utilize the stimuli (24*2 emotionally-valenced passages) [as in the original study](https://github.com/psych251/child2018_rescue/blob/main/original_paper/supplemental_materials.pdf). Alike the replication study by Bunderson (2020), all stimuli and questionnaires will be administered through Qualtrics. In light of the failed replication attempt based on a sample of 40 participants, a larger sample will be used. Participants will be recruited online through the crowdsourcing platform, Prolific. Participants will additionally asked to report the number of years of formal education they have had to facilitate exploratory analyses on the possible moderating role of educational status.

One challenge of the current project lies in the data analysis. Specifically, the main analysis used in the original study by Child et al. (2018) was a linear mixed effects (LME) model, while the main finding that Bunderson (2020) attempted to replicate was the t-test result showing a significant difference in reading time between the positively-valenced passages that were written from different perspectives (i.e., the simple effect within positively-valenced passages). It may be possible that the LME could indicate a significant interaction effect between emotional valence and perspective, but no simple effect (as in the replication study). In such a case, it would be difficult to determine if the replication should be considered a successful one or not. Additionally, another challenge for the current project is in estimating how large the possible moderation effect will be. Given that the moderation effect is a novel proposition, I may inaccurately estimate the true effect of the moderation and thus fail to find a statistically significant effect even if there was a true effect.

## Summary of prior replication attempt

There were several differences in the replication attempt done by Bunderson and the original study done by Child and colleagues that should be discussed. Firstly, as noted above, while Child and colleagues utilized a sample of college students (aged between 18 and 33), the replication study by Bunderson recruited participants (aged between 24 and 70) that may or may not have been to college. Given that college students may have greater exposure to reading and writing personal anecdotes relative to the general population, the difference in the samples' educational background may account for why the main finding that individuals read quicker when positive stories are presented in the first-person perspective was not replicated.

Secondly, there were some differences in the materials used in the studies. While the main stimuli (the emotionally-valenced passages) and the filler passages presented to participants were similar across both studies, the instructions for the tasks, the passages used for the practice trials, and the phrasing of the items used to examine participants' emotional state may have been slightly different given that Bunderson was unable to obtain these materials from the original authors. However, as the main effect we are interested in replicating is the reading time for the emotionally-valenced passages (for which were similar across both studies), I posit that these differences were unlikely to have a substantial impact on the findings of the replication study done by Bunderson. 

Thirdly, the replication study and the original study differed in the setting by which the studies were conducted. As a result, the subsequent exclusion criteria used in both studies were also different. In the original study, participants completed the study in-person on a computer using the E-Prime software. However, in the replication study, participants completed the study at their own time online using the online survey platform, Qualtrics. This is notable given that the key dependent measure (reading time) may be influenced by how immersed individuals are in the task and how conducive their physical surroundings are. Bunderson argued that external distraction was likely to be more present when a survey is completed in a participant’s personal setting rather than in a laboratory (for an alternative perspective, see [Hauser & Schwarz, 2016](https://doi.org/10.3758/s13428-015-0578-z)). Thus, to account for this, Bunderson employed two additional exclusion criteria for their data. First, Bunderson excluded participants who had reading times less than 400ms for more than half of the trials based on the assumption that these participants were likely to have been using bots or were providing 'key-smashing’ responses. Second, reading times that were longer than 15 seconds on any individual sentence of the emotionally-valenced passages were excluded based on the assumption that participants were distracted during that particular trial. While these differences in exclusion criteria may have influenced the replication of the original study, I posit that the additional exclusion criteria imposed by Bunderson were justified. Nonetheless, to examine if the results were influenced by these differences in exclusion criteria, sensitivity analyses will be conducted to ensure that the conclusions of the current replication study do not change as a result of these additional exclusion criteria.

## Methods

### Power Analysis

The original study utilized a sample of 36 participants, and the first replication utilized a sample of 40 participants. Given that the statistical tests of interest for the current replication (a pairwise test for an interaction term in a mixed effects model) is relatively complex, it was difficult to conduct an a priori power analysis to determine the sample size needed for the rescue replication. Instead, the current rescue used the rule-of-thumb suggestion of recruiting a sample approximately 2.5 times greater than the original sample (Simonsohn, 2015). Hence, 100 participants (40*2.5; using the larger replication sample size of 40 to be more conservative) were recruited for the study.

### Planned Sample

In line with the original sample used in the original study by Child et al. (2018), participants will be limited to include only native English speakers over 18 years of age without a diagnosed reading disorder. Participants will be recruited via Prolific and these inclusion criteria will be listed in the Prolific task description. Screening questions will also be added at the very start of the Qualtrics survey such that participants who are not native English speakers, are below 18 years of age, or have been diagnosed reading disorder will have their surveys terminated.

Additionally, while Bunderson (2020) used a sample of American participants, the current replication study will use a sample of British participants (as in Child et al.'s original study) to mimic the original study as closely as possible. This criteria will be enforced using Prolific's in-built geographic filter, and through a screener question at the start of the Qualtrics survey. Individuals who indicate that they were not British will have their surveys terminated and not be allowed to participate in the study.

### Materials

#### Emotionally-Valenced Passages 

In line with the original experiment, participants were presented with 24 of the 48 possible emotionally-valenced text passages created by Child et al. (2018). The passages were obtained from the [supplemental materials](https://github.com/psych251/child2018_rescue/blob/main/original_paper/supplemental_materials.pdf) uploaded by the authors of the original study. There were 24 unique passages (12 negatively-valenced and 12 positively-valenced), each with two versions: one written in a personal, second person perspective and the other written in an onlookers’, third person perspective. Each passage was 5-9 sentences long and each passage ended with an explicit emotion word that matched the content of the passage. Thus each participant read:
- 12 negatively-valenced texts, with half written in an personal (second-person) perspective and half written in an onlooker's (third-person) perspective;
- and 12 positively-valenced texts, with half written in an personal (second-person) perspective and half written in an onlooker's (third-person) perspective.

Participants only read each novel scenario once (i.e., participants read each unique story in either the second- or third-person perspective but not both). The passages were counterbalanced by perspective, valence and gender of the character for all participants.

#### Distractor Passages 

Participants were also presented with 24 distractor passages to conceal the purpose of the study. These distractor passages were obtained by Bunderson from the senior author of the paper. Distractor passages were of comparable lengths to the main emotionally-valenced passages, and were written in third (“he/she”) or first (“I”) person perspectives. Of importance, in contrast to the experimental items, "the final sentence of the fillers did not contain an explicit emotion and the texts were therefore more ambiguous" (Child et al., 2020; p.882).

### Procedure	

The stimuli were presented on Qualtrics, an online survey platform. As in the original experiment by Child et al. (2018), the sentences were presented on white background with font size 24 and black font.

The links to the paradigms used can be accessed here:
- Pilot A (Note: Passages were written in American spelling for Pilot A): [QSF file](https://github.com/psych251/child2018_rescue/blob/main/materials/child2018_pilotA.qsf), [Survey link](https://stanforduniversity.qualtrics.com/jfe/form/SV_ba0SVasU56evHy6), [Data collected](https://github.com/psych251/child2018_rescue/blob/main/data/child2018_pilotA_October%2B29%2C%2B2023_22.37.csv)
- Pilot B: [QSF file](https://github.com/psych251/child2018_rescue/blob/main/materials/child2018_pilotB.qsf), [Survey link](https://stanforduniversity.qualtrics.com/jfe/form/SV_da8ns38uiZ99Ouy), [Data collected](https://github.com/psych251/child2018_rescue/blob/main/data/child2018_pilotB_December%2B3%2C%2B2023_00.04.csv)

At the very beginning of the study, participants were presented with an introduction and three practice passages created by Bunderson (the original materials for these particular materials were not accessible). To ensure the validity of the introduction and trial passages, "Feedback was obtained during piloting to ensure that the instructions and practice trials prepared participants for the task" (Bunderson, 2020).

After completing the trial, the main text passages (the emotionally-valenced and distractor passages) were presented sentence by sentence to participants. Passages appeared in random orders for each participant and the time participants took to read each sentence was recorded. As in the original study, "After having read the final sentence, participants typed in their self-rating i.e. the number rating their own emotion" (Child et al., 2020; p.882). Specifically, in line with Bunderson's replication study, participants were asked to "*Please rate your current emotional state.*" on a 10-point Likert scale (1 = *Negative*, 10 = *Positive*). After reporting their current emotional state, the next trial began after a two second break.

After all passages were presented, participants were asked to report their years of formal education at the very end of the survey. This was done to facilitate the novel hypothesis that educational status would moderate the relationship between the perspective by which the passage was written and reading times of positively-valence passages. Specifically, I sought to examine if the effect of perspective on reading time would be larger with increasing years of formal education.

### Controls

As in the original study by Child et al. (2018) and as in Bunderson's (2020) replication study, "outliers 2.5 standard deviations below or above the mean reading times per sentence, were removed for each participant" (Child et al., 2018; p.882). Additionally, in line with Bunderson (2020), participants who had reading times less than 400ms for more than half of the trials and individual reading times that were longer than 15 seconds on any individual sentence of the emotionally-valenced passages were excluded from analysis. This is to ensure that particiapnts retained for analyses were not using bots and to ensure that reading times were not skewed by short moments of distraction.

However, to mimic the original study by Child et al. (2018) as closely as possible, sensitivity analyses will be conducted including participants who had reading times less than 400ms for more than half of the trials and including reading times that were greater than 15 seconds. Unless otherwise stated, the results presented were similar including these participants and data points.


### Analysis Plan

As in Child et al. (2018; p.882), "The reading times were analysed using linear mixed effect (LME) models. A natural log transformation was performed in order to normalise the data. We accounted for length effects of different passages by regressing (log) reading times against the number of characters per sentence. These regressions were calculated by participant. As a result, log-residual reading times with a mean (intercept) of zero (per participant) were entered into the analyses... Perspective as well as valence were included as fixed-effects into the analysis using deviation coding. In addition, we also included participants and items with random intercepts and slopes into the mixed models. For parameter reports, we used the default restricted maximum likelihood estimations provided by the lme4 package". Based on Akaike Information Criterion (AIC) scores, Child et al. (2018) found that the model including random intercepts of participants and items, but not random slopes, showed the best fit. Thus, the current analysis will utilize a model with random intercepts for participants and items but without random slopes as in the original study, unless the model fails to converge and needs to be further simplified. For this analysis, the current replication sought to examine if there would be an interaction between valence and perspective, whereby differences in reading times based on perspective were dissimilar for positively- and negatively-valenced passages.

The main result that the present study seeks to replicate is the finding that "reading times were faster for passages that were written from a personal perspective than from an onlooker perspective" (Child et al., 2018; p.883). To replicate this finding, a pairwise comparison will be conducted to examine if within the positively-valenced passages, reading times differed by the perspective the passages were written in. In the original study, the authors found that "*for positive emotions, reading times were faster for passages that were written from a personal perspective (M = 2201, SD = 1284) than from an onlooker perspective (M = 2350, SD = 1351; t(4408) = 3.73, p < .001; see Figure 1)*" (Child et al., 2018). In the replication study, Bunderson however found no significant difference in the t-test (t(934) = -0.99, p = 0.323; Bunderson, 2020). 

An exploratory analysis will also be conducted to examine if educational status would affect the strength of the relationship between perspective of passage and reading time observed within the positively-valenced passages. LME will be used given that participants' educational status is a between-persons variable while the perspective of passages was a repeated measure within participants. As in the LME analysis described above, random intercepts of participants and items will be included but not random slopes.

All analyses will be conducted in R version 4.2.2 ([Henry & Wickham, 2023](https://CRAN.R-project.org/package=rlang)). LME will be conducted using *lme4* version 1.1-33 ([Bates et al., 2015](https://doi.org/10.18637/jss.v067.i01)) and *lmerTest* version 3.1-3 ([Kuznetsova et al., 2017](https://doi.org/10.18637/jss.v082.i13)). In line with the original study, pairwise tests will be conducted using the lsmeans package version 2.30-0 ([Lenth, 2016](https://doi.org/10.18637/jss.v069.i01)).

### Differences from Original Study and 1st replication

The current replication study sought to minimize the differences from the original study as much as possible. Nonetheless, there are three differences worth nothing. Firstly, like Bunderson (2020), the instructions, trial passages, and item used for self-reported emotion may be dissimilar from the original Child et al. (2018) study given that these materials were not attainable from the original authors. The current work utilizes the materials created by Bunderson, who had re-created these materials based on the descriptions provided in the original paper, and had piloted these materials beforehand. It is expected that these changes will have minimal impact on the findings of the study given that they are not the key measures and stimuli of interest.

Secondly, the current replication study utilized a British sample so as to minimize the differences between the current study and the original study, while Bunderson's replication used an American sample. These differences in sample nationality are not expected to have a substantial impication on the findings however, given that the original study expects the effects to be applicable for readers in general and not just readers in a specific country. 

Lastly, the current study followed the exclusion criteria outlined by Bunderson (2020), which included two additional exclusion criteria compared to the original study done by Child et al. (2018) as outlined above. To ensure that these differences in exclusion criteria did not influence the findings, sensitivity analyses were conducted whereby only the exclusion criteria in the original Child et al. (2018) study was implemented. Unless otherwise stated, there were no differences in the conclusions drawn after including these participants and data points.

### Methods Addendum (Post Data Collection)

[not updated] You can comment this section out prior to final report with data collection.

#### Actual Sample

[not updated] Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan

[not updated] Any differences from what was described as the original plan, or “none”.

## Results


### Data preparation

[prose description not updated] Data preparation following the analysis plan.
	
```{r admin, include = F}
### Data Preparation: 

#### Load Relevant Libraries and Functions

library(RCurl)        # for reading data from github
library(tidyverse)    # for data cleaning
library(matrixStats)  # for data cleaning
library(psych)        # for descriptives
library(lme4)         # for LME analyses
library(lmerTest)     # for LME analyses
library(lsmeans)      # for LME (contrasts) analyses
library(ggplot2)      # for graphs
library(sessioninfo)  # for recording session info
library(pander)       # for recording session info

#### Import data and remove unnecessary columns and rows

data.url <- "https://raw.githubusercontent.com/psych251/child2018_rescue/main/data/child2018_pilotB_December%2B3%2C%2B2023_00.04.csv" 
charCount.url <- "https://raw.githubusercontent.com/psych251/child2018_rescue/main/data/Child_Oakhill_Garnham_2018_Sentence_CharacterCount.csv" 

child2018res_data <- read.csv(text = getURL(data.url), na.strings=c("",NA_real_)) %>%
  dplyr::filter(Finished == 1) %>%
  select( -c("StartDate":"RecordedDate",
             "RecipientLastName":"UserLanguage"),
         -starts_with("PRACTICE"),
         -starts_with("FILLER"),
         -contains("Click")) %>%
  rename("Subject" = "ResponseId",
         "Age" = "D1.AGE",
         "Education" = "EDU") %>%
  dplyr::select(Subject, Age, Education, everything()) %>%
  rename_all(~str_replace_all(., "_Page.Submit", ""))  %>%
  rename_all(~str_replace_all(., "X", "")) 

child2018res_charcount <- read.csv(text = getURL(charCount.url))

#### Data exclusion: removing participants

# remove participants who did not complete main survey (i.e., screened out)

filtered_data = child2018res_data %>%
  filter(D2.NATIVE.ENGLISH == 1 & D3.READING.DISORDER == 2) %>%
  dplyr::select(-D2.NATIVE.ENGLISH, -D3.READING.DISORDER)

# remove participants who spend less than 400ms on more than half of trials

filtered_data = filtered_data %>%
  dplyr::select(Subject:Education, 
                ends_with("RT"), 
                ends_with("ER")) %>% # rearrange columns
  dplyr::mutate(proplessthan400ms = rowSums(across(`2p.N1.1RT`:`3pF.P12.7RT`) < 0.400, na.rm = TRUE) / rowSums(across(`2p.N1.1RT`:`3pF.P12.7RT`) > 0.000, na.rm = TRUE)) %>% 
  dplyr::filter(proplessthan400ms < 0.5) %>%
  dplyr::select(-proplessthan400ms)

#### Data exclusion: removing specific data points

  # initial no. of datapoints = # update here #

  sum(!is.na(filtered_data %>% dplyr::select(ends_with('RT')))) 
  
  # Exclude reading times that are longer than 15 seconds & equal to 0s

filtered_data = filtered_data %>%
  dplyr::mutate_at(vars(ends_with('RT') | ends_with('ER') | contains("Edu")), as.numeric) %>%
  dplyr::mutate_at(vars(ends_with('RT')), ~ifelse(. > 15, NA_real_, .)) %>%
  dplyr::mutate_at(vars(ends_with('RT')), ~ifelse(. == 0, NA_real_, .))

  sum(!is.na(filtered_data %>% dplyr::select(ends_with('RT')))) # no. of datapoints left = 50782 (i.e., 82 datapoints removed)
  
  # Exclude reading times that are more than 2.5sd from each individuals' mean
  
filtered_data = filtered_data %>%
  dplyr::mutate(
    RTmean = rowMeans(as.matrix(filtered_data %>% dplyr::select(ends_with('RT'))), na.rm = TRUE),
    RTsd = rowSds(as.matrix(filtered_data %>% dplyr::select(ends_with('RT'))), na.rm = TRUE)) %>%
  dplyr::mutate_at(
    vars(ends_with('RT')), ~case_when(. > RTmean + (2.5 * RTsd) ~ NA_real_,
                                      . < RTmean - (2.5 * RTsd) ~ NA_real_,
                                      TRUE ~ .)) %>%
  dplyr::select(-RTmean, -RTsd)

  sum(!is.na(filtered_data %>% dplyr::select(ends_with('RT')))) 
    # new no. of datapoints = # update here # (i.e., # update here # datapoints removed)
  
#### Prepare data for analysis
  
  # Convert data into long format (RT; Main data for replication)
  
  long_RT_data = filtered_data %>% 
    dplyr::select(-ends_with(".ER")) %>%
    pivot_longer(cols=-c("Subject", "Age", "Education"),
                 names_to = 'Passage',
                 values_to = 'RT') %>% 
    dplyr::mutate(Perspective = as.factor(grepl("2p", Passage)),
                  Valence = as.factor(grepl(".P", Passage))) %>%
    dplyr::filter(!is.na(RT)) %>%
    left_join(., child2018res_charcount, by = c("Passage")) %>%
    dplyr::mutate(log_RT = log(RT)) # create log RT
  
  # Convert data into long format (ER; Exploratory data)
  
  long_ER_data = filtered_data %>% 
    dplyr::select(-ends_with("RT")) %>%
    pivot_longer(cols=-c("Subject", "Age", "Education"),
                 names_to = 'Passage',
                 values_to = 'ER') %>% 
    mutate(Perspective = as.factor(grepl("2p", Passage)),
           Valence = as.factor(grepl(".P", Passage))) %>%
             dplyr::filter(!is.na(ER))

```

### Results of control measures

There were three exclusion criteria used. Firstly, participants (*n* = ?? participants) who had reading times of less then 400ms for more than half the trials were excluded from analysis. Secondly, reading times that were longer than 15 seconds were re-coded as missing data (*n* = ?? data points across ?? participants; ??.??% of datapoints). Finally, outliers 2.5 standard deviations below or above the mean reading times per sentence per participant were removed (*n* = ?? data points; ??.??% of datapoints). Thus, a total of ?? participants (??.??% missing reading time data) were retained for analysis.

### Confirmatory analysis

The basic descriptive statistics of the sample (age, years of formal education, emotion rating responses, average raw reading times) are presented below.

```{r descriptives, include=T}
### participant level statistics of the sample

psych::describe(filtered_data %>%
                  dplyr::mutate(
                    ReadingTime = rowMeans(as.matrix(filtered_data %>% dplyr::select(ends_with('RT'))), na.rm = TRUE),
                    EmotionRating = rowMeans(as.matrix(filtered_data %>% dplyr::select(ends_with('ER'))), na.rm = TRUE)) %>%
                  dplyr::select(Age, Education, ReadingTime, EmotionRating))

```

In line with the original study and the replication study done by Bunderson, log-residual reading times were calculated to account for the length effects of different items. A regression of (log-transformed) reading times against the number of characters per sentence was performed and the log-residual reading times (with an intercept of zero for each participant) were used for the main analyses later on. 

```{r log-residual-calculation, include=T}
### Regress log RT against character count and create variable

reg_RT_data = lmer(log_RT ~ 1 + Character_Count + (1 + Character_Count | Subject), long_RT_data)
long_RT_data$log_RT_Resid <- residuals(reg_RT_data)
rm(reg_RT_data)

```

In line with the original study, a LME model was run whereby random intercepts for both participants and items were included in the model. Results show that ...

```{r include=T}
### analyses: LME and contrasts

 RT_model <- 
  lmer(log_RT_Resid ~ Perspective + Valence + Perspective*Valence + 
         (1 | Subject) + (1 | Passage), 
       data = long_RT_data)
  summary(RT_model, ddf = "Satterthwaite")

```

To test if participants read positively-valenced passages faster when they were written from a personal (second person's) perspective compared to when they were written from an onlooker's (third person's) perspective, pairwise comparisons were conducted using the *lsmeans* package, as in the original study. Results show that ...

```{r include=T}
### lsmeans to text for pairwise contrasts 

  RT_model_con <- 
    lsmeans(RT_model, 
            pairwise ~ Perspective | Valence, 
            at = list(Perspective = c("TRUE", "FALSE"), Valence = c("TRUE", "FALSE")),
            pbkrtest.limit = 5100) # WARNING: This line is computationally heavy and takes a while to run
  
  contrast(RT_model_con, method = "pairwise", ddf = "Satterthwaite")
  
# graph RT results
  
  RT_data_graph = long_RT_data %>%
    group_by(Perspective, Valence) %>%
    summarise(Mean_logRTResid = mean(log_RT_Resid, na.rm = TRUE),
              SE_RT = sd(log_RT_Resid, na.rm = TRUE) / sqrt( length(log_RT_Resid[!is.na(log_RT_Resid)])))

ggplot(data = RT_data_graph, 
       aes(x = Valence, 
           y = Mean_logRTResid, 
           fill = Perspective)) +
  geom_bar(position = "dodge", 
           stat = "identity") +
  geom_errorbar(aes(ymin = Mean_logRTResid - SE_RT, 
                    ymax = Mean_logRTResid + SE_RT), 
                width = .5, position=position_dodge(.9), 
                na.rm = TRUE)  +
  labs(title = "Reading Times ",
       subtitle = "for Valence by Perspective",
       tag = "A",
       y = "Log Residual Reading Times", 
       x = "Valence") +
  scale_x_discrete(labels=c("FALSE"="Negative", 
                            "TRUE"="Positive")) +
  scale_fill_discrete(name="Perspective", 
                      labels = c("Third-person (he/she)",
                                 "Second-person (you)"))

knitr::include_graphics("https://raw.githubusercontent.com/psych251/child2018_rescue/main/original_paper/child2018_figure1.png?raw=TRUE")

  # clean environment
  rm(RT_model, RT_model_con, RT_data_graph)
  
```

### Exploratory analyses

To explore if educational status moderated the relationship between perspective of the passage and reading time among positively-valenced passages, a LME was conducted and an interaction term between the years of formal education and perspective was included as a predictor variable in the model. Results show that ...

```{r include=T}
### analyses: education as moderator

 RT_model <- 
  lmer(log_RT_Resid ~ Perspective + Valence + Perspective*Valence*Education + 
         (1 | Subject) + (1 | Passage), 
       data = long_RT_data)
  summary(RT_model, ddf = "Satterthwaite")
```

## Discussion

## Mini meta analysis
Combining across the original paper, 1st replication, and 2nd replication, what is the aggregate effect size? 

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.